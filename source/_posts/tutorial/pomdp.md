---
title: pomdp tutorial
date: 2018-08-01 09:00:00
comments: true
tags:
     - pomdp 
categories: 
    - tutorial
---


We are attempt to implement four method as solver
* stack-DQN
* Vaule Itervation
* POMCP
* Deep Recurrent Q network (DRQN)  

[1] [summary of current POMDP solver](https://bayesgroup.github.io/bmml_sem/2018/Shvechikov_Partially%20Observable%20Markov%20Decision%20Process%20in%20Reinforcement%20Learning.pdf)  
[2] [the vi+pomcp solver source code.](https://github.com/pemami4911/POMDPy)  
[3] POMCP - Silver, David, and Joel Veness. "Monte-Carlo planning in large POMDPs." Advances in neural information processing systems. 2010.  
[4] UCT, kenerl of POMCP - L. Kocsis and C. Szepesvari. Bandit based Monte-Carlo planning.  In 15th European Conference on Machine Learning, pages 282â€“293, 2006.  
[5] [slides of CMU](https://www.cs.cmu.edu/~ggordon/780-fall07/lectures/POMDP_lecture.pdf)  
[6] [slides of techfak](https://www.techfak.uni-bielefeld.de/~skopp/Lehre/STdKI.../POMDP_tutorial.pdf)   
[7] [pomdp alg website](http://www.pomdp.org/)  
[8] [DRQN blog](https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-6-partial-observability-and-deep-recurrent-q-68463e9aeefc)  
[9] DRQN paper - Hausknecht, Matthew, and Peter Stone. "Deep recurrent q-learning for partially observable mdps." CoRR, abs/1507.06527 (2015). 